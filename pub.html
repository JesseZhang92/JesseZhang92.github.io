
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="nju.css" type="text/css" />
<title>Selected Pulications </title>
</head>
<body>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="image-container">
    <img src="./project/1200px-NJU.svg.png" width="90px" height="112px" alt="NJU">
</div>
<div class="menu-item"><a href="index.html">Homepage</a></div>
<div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
<div class="menu-item"><a href="group.html">Members</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<!-- <div class="menu-item"><a href="join.html">Join&nbsp;us</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Pulications </h1>
</div>
<p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <b><span style="font-family:Wingdings">*</span></b> corresponding author. All publications on [<a href="https://scholar.google.com/citations?user=4daxK2AAAAAJ" target=&ldquo;blank&rdquo;>Google Scholar</a>])
<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./project/ImAR-arXIv2023.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="https://arxiv.org/pdf/2203.08612.pdf">
<papertitle><b>Learning Versatile 3D Shape Generation with Improved AR Models</b></papertitle>
</a>
<br>
S. Luo, X. Qian, Y. Fu, Y. Zhang, Y. Tai, <b>Z. Zhang</b>, C. Wang, and X. Xue.              
<br>
<em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
<br>
<a href="https://arxiv.org/pdf/2303.14700.pdf"; style="color: #EE7F2D;">arXiv</a>
<p></p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./project/NPF.png" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
<a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf">
<papertitle><b>Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild</b></papertitle>
</a>
<br>
<b>Z. Zhang</b>, R. Chen, W. Cao, Y. Tai<b><span style="font-family:Wingdings">*</span></b>, and C. Wang<b><span style="font-family:Wingdings">*</span></b>
<br>
<em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
<br>
<a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf"; style="color: #EE7F2D;">Paper</a> 
/
<a href="https://www.youtube.com/watch?v=_Du5z4qKT0w">Video</a> 
<p><font color="red">State-of-the-art 3D portrait modeling, robust to large pose, challenging light or makeup. Also state-of-the-art novel view synthetis performance.</font></p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/rignet.png" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2107.13802.pdf">
    <papertitle><b>RigNet: Repetitive Image Guided Network for Depth Completion</b></papertitle>
    </a>
    <br>
    Zhiqiang Yan, Kun Wang, Xiang Li, <b>Zhenyu Zhang</b>, Jun Li and Jian Yang
    <br>
    <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2107.13802.pdf"; style="color: #EE7F2D;">Paper</a> 
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/mpt.png" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2203.09855.pdf">
    <papertitle><b>Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion</b></papertitle>
    </a>
    <br>
    Zhiqiang Yan, Xiang Li, Kun Wang, <b>Zhenyu Zhang</b>, Jun Li and Jian Yang
    <br>
    <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022
    <br>
    <a href="https://arxiv.org/pdf/2203.09855.pdf"; style="color: #EE7F2D;">Paper</a> 
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
<img src="./project/PhyDIR1.PNG" style="height: 120px; width: 200px; margin-top: 10px">
<td style="padding:10px;width:75%;vertical-align:middle">
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Physically-Guided_Disentangled_Implicit_Rendering_for_3D_Face_Modeling_CVPR_2022_paper.pdf">
    <papertitle>Physically-Guided Disentangled Implicit Rendering for 3D Face Modeling</papertitle>
  </a>
  <br>
      <b>Zhenyu Zhang</b>, Yanhao Ge, Ying Tai, Weijian Cao, Renwang Chen, Kunlin Liu, Hao Tang, Xiaoming Huang, Chengjie Wang, Dongjin Huang, Zhifeng Xie.              
      <br>
      <em><b>CVPR</b></em> 2022
  <br>
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Physically-Guided_Disentangled_Implicit_Rendering_for_3D_Face_Modeling_CVPR_2022_paper.pdf">Paper</a> /
  <p><font color="red">3D face modeling is limited by classical graphics rendering, so that we let it benefit from a novel neural rendering approach.</font></p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
  <img src="./project/L2R.PNG" style="height: 120px; width: 200px; margin-top: 10px">
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Learning_To_Restore_3D_Face_From_In-the-Wild_Degraded_Images_CVPR_2022_paper.pdf">
    <papertitle><b>Learning to Restore 3D Face from In-the-Wild Degraded Images</b></papertitle>
    </a>
    <br>
    <b>Zhenyu Zhang</b>, Yanhao Ge, Ying Tai, Xiaoming Huang, Chengjie Wang, Hao Tang, Dongjin Huang, Zhifeng Xie.              
    <br>
    <em><b>CVPR</b></em> 2022
    <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Learning_To_Restore_3D_Face_From_In-the-Wild_Degraded_Images_CVPR_2022_paper.pdf">Paper</a> /
    <p><font color="red">When restoring degraded faces, you need to restore the 3D geometry-aware effect.</font></p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle"> 
            
    <img src="./project/ICCV21.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2108.03830">
      <papertitle><b>Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark</b></papertitle>
      </a>
      <br>
      Kun Wang*, <b>Zhenyu Zhang*</b>, Xiang Li, Jun Li, Baobei Xu, Jian Yang.              
      <br>
      <em><b>ICCV</b></em> 2021
      <br>
      <a href="https://arxiv.org/pdf/2108.03830.pdf">Paper</a> /
      <a href="https://github.com/w2kun/RNW">Code</a> 
      <p><font color="red">Nighttime is challenging, but we make it. Dataset and code are now released.</font></p>
</td><tr>

<td style="padding:20px;width:25%;vertical-align:middle"> 
            
  <img src="./project/LAP2.PNG" style="height: 120px; width: 200px; margin-top: 10px">  
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="http://arxiv.org/abs/2106.07852">
      <papertitle><b>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</b></papertitle>
    </a>
    <br>
        <b>Zhenyu Zhang</b>, Yanhao Ge, Renwang Chen, Ying Tai, Yan Yan, Jian Yang, Chengjie Wang, Jinlin Li and Feiyue Huang.              
        <br>
        <em><b>CVPR</b></em> 2021, &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
    <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf">Paper</a> /
    <a href="http://arxiv.org/abs/2106.07852">Arxiv</a> /
    <a href="https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP">Code</a> 
    <p><font color="red">Improving the non-parametric 3D face reconstruction by leveraging consistency of unconstrained photo collection.</font>
      
    </p>
  </td>
<tr>

  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/AAAI20_2.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2003.11228.pdf">
      <papertitle><b>Cross-modal attention network for temporal inconsistent audio-visual event localization</b></papertitle>
      </a>
      <br>
      Hanyu Xuan, <b>Zhenyu Zhang</b>, Shuo Chen, Jian Yang, Yan Yan.              
      <br>
      <em><b>AAAI</b></em> 2020
      <br>
      <p>Audio-visual event localization on temporal inconsistent videos</p>
    </td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/PSD.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Pattern-Structure_Diffusion_for_Multi-Task_Learning_CVPR_2020_paper.pdf">
      <papertitle><b>Pattern-Structure Diffusion for Multi-Task Learning</b></papertitle>
      </a>
      <br>
      Ling Zhou, Zhen Cui, Chunyan Xu, <b>Zhenyu Zhang</b>, Chaoqun Wang, Tong Zhang, Jian Yang.              
      <br>
      <em><b>CVPR</b></em> 2020
      <br>
      <p>A graph-based method to mine multi-task relationship.</p>
    </td><tr>
  
<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/LPF2.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf">
      <papertitle><b>Online Depth Learning against Forgetting in Monocular Videos</b></papertitle>
      </a>
      <br>
      <b>Zhenyu Zhang</b>, Stephane Lathuiliere, Elisa Ricci, Nicu Sebe, Yan Yan, Jian Yang.              
      <br>
      <em><b>CVPR</b></em> 2020
      <br>
      <p><font color="red">Depth estimation method fails in new scenes, but we can continuously align it against domain shift.</font></p>
    </td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/PAP.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Pattern-Affinitive_Propagation_Across_Depth_Surface_Normal_and_Semantic_Segmentation_CVPR_2019_paper.pdf">
      <papertitle><b>Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation</b></papertitle>
      </a>
      <br>
      <b>Zhenyu Zhang</b>, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, Jian Yang.              
      <br>
      <em><b>CVPR</b></em> 2019
      <br>
      <p><font color="red">Feel difficult to combine different tasks? Here we provide a pair-wise similarity based method to leverage multi-task correlation.</font></p>
    </td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/TRL_PAMI.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://ieeexplore.ieee.org/abstract/document/8758995">
      <papertitle><b>Joint Task-Recursive Learning for RGB-D Scene Understanding</b></papertitle>
      </a>
      <br>
      <b>Zhenyu Zhang</b>, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, Jian Yang.              
      <br>
      <em><b>TPAMI</b></em> 2019
      <br>
      <p>A recursive approach for joint-task learning in RGBD scenes.</p>
    </td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/ECCV.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenyu_Zhang_Joint_Task-Recursive_Learning_ECCV_2018_paper.pdf">
      <papertitle><b>Joint Task-recursive Learning for Semantic Segmentation and Depth Estimation</b></papertitle>
      </a>
      <br>
      <b>Zhenyu Zhang</b>, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, Jian Yang.              
      <br>
      <em><b>ECCV</b></em> 2018
      <br>
      <p>A new framework for joint depth estimation & semantic segmentation.</p>
    </td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/TIP.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://ieeexplore.ieee.org/abstract/document/8331148">
      <papertitle><b>Progressive Hard-Mining Network for Monocular Depth Estimation</b></papertitle>
      </a>
      <br>
      <b>Zhenyu Zhang</b>, Chunyan Xu, Jian Yang, Junbin Gao, Zhen Cui.              
      <br>
      <em><b>TIP</b></em> 2018
      <br>
      <p>Improving monocular depth estimation by mining difficult regions.</p>
    </td><tr>

<td style="padding:20px;width:25%;vertical-align:middle">
    <img src="./project/pr.JPG" style="height: 120px; width: 200px; margin-top: 10px">
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320318301869">
      <papertitle><b>Deep hierarchical guidance and regularization learning for end-to-end depth estimation</b></papertitle>
      </a>
      <br>
      <b>Zhenyu Zhang</b>, Chunyan Xu, Jian Yang, Ying Tai, Liang Chen.              
      <br>
      <em><b>Pattern Recognition</b></em> 2018
      <br>
      <p>A new framework for monocular depth estimation.</p>
    </td><tr>


</table>

</td>
</tr>
</table>
</body>
</html>