<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="nju.css" type="text/css" />
<title>Zhenyu Zhang's Homepage</title>
</head>
<body>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="image-container">
    <img src="./project/1200px-NJU.svg.png" width="90px" height="112px" alt="NJU">
</div>
<div class="menu-item"><a href="index.html" class="current">Homepage</a></div>
<div class="menu-item"><a href="pub.html">Publications</a></div>
<div class="menu-item"><a href="group.html">Members</a></div>
<div class="menu-item"><a href="service.html">Services</a></div>
<div class="menu-item"><a href="award.html">Awards</a></div>
<!-- <div class="menu-item"><a href="join.html">Join&nbsp;us</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Zhenyu Zhang's Homepage</h1>
</div>
<table class="imgtable"><tr><td>
<img src="./project/Zhenyu_square.jpg" alt="Zhenyu Zhang's profile picture" width="270px" height="297px" />&nbsp;</td>
<td align="left"><p><b>Dr. Zhenyu Zhang</b>
</p>
<p><b>Associate Professor</b>
</p>
<p><a href="https://njusz.nju.edu.cn/main.htm" target=&ldquo;blank&rdquo;>Nanjing University (Suzhou Campus)</a>
</p>
<p>1520 Taihu Road, Suzhou, P.R. China
</p>
<p><b>Email:</b> zhangjesse(at)foxmail.com
</p>
<p><a href="https://scholar.google.com/citations?user=4daxK2AAAAAJ" target=&ldquo;blank&rdquo;>Google Scholar</a> | 

  <a href="project/CV_Zhenyu_Zhang3.pdf">CV</a> 
</p>
</td></tr></table>

<div class="infoblock">
<div class="blocktitle"></div>
<div class="blockcontent">
<p>Looking for self-motivated graduate students (both Ph.D. and master) working with me. For the prospective students, please send me your resume. 
</p>
<p><span lang="zh-CN" xml:lang="zh-CN"> Opening positions: 正在招收2026年秋季入学博士，硕士。名额充足，欢迎有3D视觉/生成式AI基础，或对相关方向感兴趣，有一定论文成果发表经历（CCF-A为佳），致力于做出有影响力工作的同学尽早联系我。课题组也同时招聘RA（研究助理）2名，访问学生多名，方向为空间智能与数字人，有充足经费与计算资源，全力支持成果发表。有希望合作研究的同学也欢迎与我联系，谢谢！
  
</span>
</p>
</div></div>
<h2>Biography</h2>
<p> I am currently an associate professor at school of Intelligent Science and Technology, Nanjing University, Suzhou campus, 
  where I work on computer vision, computer graphics and machine learning.

  I got my Ph.D degree from Department of Computer Science and Engineering, Nanjing University of Science and Technology in 2020, 
  supervised by <b><a href="https://scholar.google.com/citations?user=6CIDtZQAAAAJ">Jian Yang</a></b>. 
  In 2019, I spent 10 wonderful months as a visiting student at <b><a href="http://mhug.disi.unitn.it/index.php/publications/#/">MHUG</a></b> group in Unviversity of Trento, Italy, supervised by <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a>. 
  During 2020-2023, I worked as a staff research scientist at Tencent Youtu Lab.
</p>
<p>My research interests include 3D modeling, rendering, frontier generative AI research / applications based on advanced large vision and language models. My long-term research objective is to create an interacitve world simulator from real-world knowledge and human prior.
  Specifically, I work on 
</p>
<ul>
<li><p>3D digital human  
</p>
</li>
<li><p>3D Content Generation
</p>
</li>
<li><p>Spatial Intelligence
</p>
</li>
</ul>
<h2>News</h2>
<ul>
  <li><p style="text-align:left">07/2025 – 1 paper has been accepted by <b>TPAMI</b>.  </p></li>
  <li><p style="text-align:left">06/2025 – 3 papers are accepted by <b>ICCV'25</b>.  </p></li>
  <li><p style="text-align:left">04/2025 – I obtain Zijin Scholar of Nanjing University.  </p></li>
  <li><p style="text-align:left">12/2024 – Our paper <font color="blue"><b>StrandHead</b></font> for Hair-disentangled 3D potrait generation has been released. Here is the <b><a href="https://xiaokunsun.github.io/StrandHead.github.io/">Link</a></b>  </p></li>
  <li><p style="text-align:left">10/2024 – I am invited by CCF (technical commitee on computer vision) to give a talk on the seminar "Human-centric Visual Perception". Here is the <b><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDU3MTc1Ng==&mid=2651183553&idx=1&sn=bac2c009421c55f5391544b20bb3c025&chksm=85735f2a9d6581606c80687cf988014c9f62771c0bbb1bba88b9312f4af39643721c184b222f&sessionid=1729736355&scene=126&clicktime=1729753092&enterid=1729753092&subscene=10000&ascene=3&fasttmpl_type=0&fasttmpl_fullversion=7440779-zh_CN-zip&fasttmpl_flag=0&realreporttime=1729753092681&devicetype=android-31&version=28003536&nettype=cmnet&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&session_us=gh_d14c0d142e9b&countrycode=CN&exportkey=n_ChQIAhIQ24UMNXgVpPdkripRTQDZYBLvAQIE97dBBAEAAAAAALmAIVdQpZEAAAAOpnltbLcz9gKNyK89dVj0xvhZ6r3mz%2BXG0N6fpaXlw7Ia5KPqyd25%2BjfvfptI7xNYVIuehjlNWDzev2V7Ov83sbT%2FJDWhlHKbkqwc0CdUoPUFAWgMbuwKwQl2vb4iih%2Fn1pRtTKn5Qef2iPtYBeBu2nV%2FRfJ0H%2Bq9pZTq5hrehrXXaELzj3CTOdCRYbWnxuJCv3OItLBg6%2BJzJyWXyEVfIkdPfK6UVxp%2FbNwauUOKlGY5U2hNfR2TZ0aqJsZrqRsnBFggHSqcn1qBHRDsrZCEQMmfqGZLJFb3&pass_ticket=gSxmgOciGP3ahppGAF6%2BQYuZf2Hvii%2BHmY0lMHaiGtj30NPGcNM09Qj%2FgGQL%2Bmu1&wx_header=3">link</a></b></p></li>
  <li><p style="text-align:left">09/2024 – One paper is accepted by NeurIPS'24 </p></li>
  <li><p style="text-align:left">09/2024 – Our paper ConsistentAvatar has been selected for <font color="red">oral representation</font> in ACM'MM 24 </p></li>
  <li><p style="text-align:left">08/2024 – Our paper <font color="blue"><b>Barbie</b></font> for disentangled 3D avatar generation has been released. Here is the <b><a href="https://xiaokunsun.github.io/Barbie.github.io/">Link</a></b> </p></li>
  <li><p style="text-align:left">07/2024 – One paper on temporal-consistent talking head generation is accepted by <b>ACMMM'24</b>. </p></li>
  <li><p style="text-align:left">06/2024 – Our paper <b><a href="https://arxiv.org/html/2403.15008v1">TPVD</a></b> is selected as an <font color="red">oral representation</font> on CVPR 2024.</p></li>
  <li><p style="text-align:left">05/2024 – I am invited by CCF to give a talk on 3D Face Reconstruction and Generation. Links can be found at CCF official account of Wechat.</p></li>
  <li><p style="text-align:left">03/2024 – Our survey and benchmark on Deepfake Generation and Detection is now released, please see <b><a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">this page</a></b>. </p></li>
  <li><p style="text-align:left">03/2024 – 1 paper <b><a href="https://arxiv.org/html/2403.15008v1">TPVD</a></b> on geometry-aware depth completion is accepted by <b>CVPR'24</b>. </p></li>
  <li><p style="text-align:left">12/2023 – 1 paper <b><a href="https://arxiv.org/pdf/2308.10001.pdf">AltNeRF</a></b> on robust 3D scene reconstruction is accepted by <b>AAAI'24</b>. </p></li>
  <li><p style="text-align:left">08/2023 – I obtain a general program from National Natural Science Foundation of China (NSFC). The program is on digitual human generation.</p></li>
  <li><p style="text-align:left">03/2023 – 1 paper <b><a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf">NPF</a></b> on 3D face modeling is accepted by <b>CVPR'23</b> </b> </p></li>
  <li><p style="text-align:left">09/2022 – 1 paper <b><a href="https://ojs.aaai.org/index.php/AAAI/article/download/25415/25187">DesNet</a></b> on depth completion is accepted by <b>AAAI'23</b> </b> </p></li>
  <li><p style="text-align:left">07/2022 – 2 papers (<b><a href="https://arxiv.org/pdf/2107.13802.pdf">RigNet</a></b> and <b><a href="https://arxiv.org/pdf/2203.09855.pdf">M^3PT</a></b> ) on depth completion are accepted by <b>ECCV'22</b> </b> </p></li>
  <li><p style="text-align:left">03/2022 – 2 papers on face reconstruction and neural rendering are accepted by <b>CVPR'22</b>, with the acceptance rate to be <b>25.3%</b> </p></li>
  <li><p style="text-align:left">03/2022 – I obtain 2021 AI Evangelist of Tencent Youtu Lab </p></li>
  <li><p style="text-align:left">01/2022 – I obtain <b>nomination award of CCF (China Computer Federation) Outstanding Doctoral Thesis</b>. See this <a href="https://www.ccf.org.cn/Awards/Awards/2022-01-05/752703.shtml">page</a> (in Chinese) </p></li>
  <li><p style="text-align:left">07/2021 – 1 paper on nighttime depth estimation accepted by <b>ICCV'21</b>, with the acceptance rate to be <b>25.9%</b> </p></li>
  <li><p style="text-align:left">07/2021 – Our <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;"> <b>ASFD</b></a> on face detection is accepted by <b>ACM MM'21</b> </p></li>
  <li><p style="text-align:left">03/2021 – 1 paper on 3D face modeling accepted by <b>CVPR'21 (Oral!)</b>, with the acceptance rate to be <b>23.7%</b> </p></li>
  <li><p style="text-align:left">02/2020 – 2 papers accepted by <b>CVPR'20</b>, with the acceptance rate to be <b>22.1%</b> </p></li>
  <li><p style="text-align:left">11/2019 – 1 paper accepted by <b>AAAI'20</b>, with the acceptance rate to be <b>20.6%</b> </p></li>
  <li><p style="text-align:left">10/2019 – Our ECCV'18 extended paper is accepted by <b>TPAMI!</b> </p></li>
  <li><p style="text-align:left">04/2019 – Our paper on online depth adaptation is released at <a href="https://arxiv.org/pdf/1904.08462.pdf">Arxiv</a> </p></li>
  <li><p style="text-align:left">03/2019 – 1 paper accepted by <b>CVPR'19</b> </p></li>
  <li><p style="text-align:left">12/2018 – I join <a href="http://mhug.disi.unitn.it/index.php/publications/#/">MHUG</a> as a visiting student. Wonderful people and city! </p></li>
  <li><p style="text-align:left">06/2018 – 1 paper accepted by <b>ECCV'18</b> </p></li>
</ul>

<br>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=230&t=n&d=jXVRFhdcBIptmA-uInIx4ipfYBGEvhLs18EfNz-o7Ro'></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
<div id="footer">
<div id="footer-text">
Last modified in May 2025, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
  
</td>
</tr>
</table>
</body>
</html>
